{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "We will begin with the binary classification problem, which can then be trivially generalized to multi-class problems using OneVsOne or OneVsAll\n",
    "## 1. Notations\n",
    "$\\DeclareMathOperator{\\argmin} {arg min}\n",
    "\\DeclareMathOperator{\\argmax} {arg max}$\n",
    "- Let $E=\\mathbb{R}^m$ the vector space a features\n",
    "- Let $P=\\mathbb{R}^{m+1}$ be a vector space in which the parameters vary\n",
    "- For $\\omega \\in P$, we will denote $w_*=(w_1,..w_n)$\n",
    "- We will denote by $\\mathbb{1}_i \\in \\mathbb{R}^i $ the vector of ones $\\forall i \\in \\mathbb{N}$ \n",
    "- Let $F\\in \\mathscr{C}^1$ a cumulative probability function\n",
    "- Let $m\\in\\mathbb{N}^*$ the number of features.\n",
    "- Let $n \\in \\mathbb{N}^*$ the number of samples\n",
    "- Let $X \\in E^n$ be a tuple of input samples \n",
    "- Let $Y \\in \\{0,1\\}^n$ be a tuple of samples \n",
    "- Let $C \\in \\mathbb{R}_+$ be a hyperparameter\n",
    "- Let $\\mathscr{N}=\\lVert\\cdot\\rVert$ be a norm on $E$\n",
    "- Let $\\mathscr{l}\\in \\mathscr{F}\\left([0,1]^2,\\mathbb{R}\\right)$ the unit cost function\n",
    "- Let $\\mathscr{L} \\in \\mathscr{F}\\left(P,\\mathbb{R}\\right)/$\n",
    "$$\\forall \\omega \\in P,\\quad \\mathscr{L}(\\omega)=C\\sum_{i=1}^{n}\\mathscr{l}\\left(F(X_i^T\\omega_*+\\omega_0),Y_i\\right)+\\lVert \\omega_*\\rVert$$\n",
    "## 2. Objective\n",
    "We will try to find: $$\\omega_m=\\arg\\min_{\\omega\\in P} \\mathscr{L}(\\omega)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.3</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "148                6.2               3.4                5.4               2.3   \n",
       "87                 6.3               2.3                4.4               1.3   \n",
       "124                6.7               3.3                5.7               2.1   \n",
       "85                 6.0               3.4                4.5               1.6   \n",
       "25                 5.0               3.0                1.6               0.2   \n",
       "\n",
       "     target       label  \n",
       "148       2   virginica  \n",
       "87        1  versicolor  \n",
       "124       2   virginica  \n",
       "85        1  versicolor  \n",
       "25        0      setosa  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "def createIRIS():\n",
    "    data=datasets.load_iris()\n",
    "    U=pd.DataFrame(data[\"data\"],columns=data[\"feature_names\"])\n",
    "    labels=pd.Series(data[\"target_names\"])\n",
    "    U[\"target\"]=data[\"target\"]\n",
    "    U[\"label\"]= [labels[i] for i in data[\"target\"]]\n",
    "    return U\n",
    "U = createIRIS()\n",
    "U.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X=U[U.columns[:-2]]\n",
    "Y=U[U.columns[-2]]\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "from scipy.special import xlogy\n",
    "import scipy\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def dsig(x):\n",
    "    S=sigmoid(x)\n",
    "    return S*(1-S)\n",
    "\n",
    "class BinaryLogisticRegression:\n",
    "    def __init__(self,C=1,p=0.5,F=sigmoid,dF=None,N=np.linalg.norm,N_args=[]):\n",
    "        self.C=C\n",
    "        self.p=p\n",
    "        self.F=F\n",
    "        self.dF=dF\n",
    "        self.cost=BinaryLogisticRegression.cost\n",
    "        self.N_args=N_args\n",
    "        self.N=lambda u:N(u,*self.N_args)\n",
    "        self.cost=BinaryLogisticRegression.cost\n",
    "    def predict(self,X):\n",
    "        return self.decision_function(X)>self.p\n",
    "    def decision_function(self,X):\n",
    "        return self.F(X@self.w[1:]+self.w[0])\n",
    "    def fit(self,X,Y):\n",
    "        self.w=optimize.minimize(lambda w:self.cost(X,Y,w,self.F,self.C,self.N),jac=None,x0=np.zeros(X.shape[1]+1)).x\n",
    "        return self\n",
    "    def parameters(self):\n",
    "        return self.w\n",
    "    @classmethod\n",
    "    def cost(cls,X,Y,w,F,C,N):\n",
    "        H=F(X@w[1:]+w[0])        \n",
    "        return -C*np.sum(xlogy(Y,H)+xlogy(1-Y,1-H))+N(w[1:])\n",
    "    pass\n",
    "\n",
    "class OneVsAllLogisticRegression:\n",
    "    def __init__(self,C=1,F=sigmoid,dF=None,N=np.linalg.norm,N_args=[],cost=BinaryLogisticRegression.cost):\n",
    "        self.C=C\n",
    "        self.F=F\n",
    "        self.dF=dF\n",
    "        self.N=N\n",
    "        self.cost=cost\n",
    "    def fit(self,X,Y):\n",
    "        X=np.array(X)\n",
    "        Y=np.array(Y)\n",
    "        self.k=np.max(Y)+1\n",
    "        self.W=np.zeros([X.shape[1]+1,self.k])\n",
    "        for i in range(self.k):\n",
    "            self.W[:,i]=BinaryLogisticRegression(C=self.C,F=self.F,dF=self.dF,N=self.N,N_args=self.N_args,cost=self.cost).fit(X,Y==i).parameters()\n",
    "        \n",
    "    def predict(self,X):\n",
    "        S=np.zeros([X.shape[0],self.k])\n",
    "        V=self.F(np.array(X)@self.W[1:,:]+self.W[0,:])\n",
    "        for s in range(X.shape[0]):\n",
    "            for i in range(self.k):\n",
    "                P=np.full(self.k,(1-V[s,i])/(self.k-1))\n",
    "                P[i]=V[s,i]\n",
    "                S[s,i]=S[s,i]+P[i]\n",
    "        return np.argmax(S,axis=1)\n",
    "    def parameters():\n",
    "        return self.W\n",
    "    pass\n",
    "        \n",
    "class OneVsOneLogisticRegression:\n",
    "    def __init__(self,C=1,F=sigmoid,dF=None,N=np.linalg.norm,N_args=[],cost=BinaryLogisticRegression.cost):\n",
    "        self.C=C\n",
    "        self.F=F\n",
    "        self.dF=dF\n",
    "        self.N=N\n",
    "        self.N_args=N_args\n",
    "        self.cost=cost\n",
    "    def fit(self,X,Y):\n",
    "        X=np.array(X)\n",
    "        Y=np.array(Y)\n",
    "        self.k=np.max(Y)+1\n",
    "        nCr=lambda a,b:int(scipy.special.comb(a,b))\n",
    "        m=nCr(self.k,2)\n",
    "        self.W=np.zeros([X.shape[1]+1,m])\n",
    "        for i in range(self.k):\n",
    "            for j in range(i):\n",
    "                mask=(Y==i)|(Y==j)\n",
    "                self.W[:,nCr(i,2)+j]=BinaryLogisticRegression(C=self.C,F=self.F,dF=self.dF,N=self.N,N_args=self.N_args,cost=self.cost).fit(X[mask],Y[mask]==j).parameters()\n",
    "        \n",
    "    def predict(self,X):\n",
    "        nCr=lambda a,b:int(scipy.special.comb(a,b))\n",
    "        m=nCr(self.k,2)\n",
    "        S=np.zeros([X.shape[0],self.k])\n",
    "        X=np.array(X)\n",
    "        V=self.F(X@self.W[1:,:]+self.W[0,:])\n",
    "        P=np.zeros(self.k)\n",
    "        for s in range(X.shape[0]):\n",
    "            for i in range(self.k):\n",
    "                for j in range(i):                    \n",
    "                    S[s,j]=S[s,j]+V[s,nCr(i,2)+j]\n",
    "                    S[s,i]=S[s,i]+1-V[s,nCr(i,2)+j]\n",
    "        return np.argmax(S,axis=1)\n",
    "    def parameters():\n",
    "        return self.W\n",
    "    pass\n",
    "\n",
    "class MultinomialLogisticRegression:\n",
    "    def __init__(self,C=1,F=sigmoid,dF=None,N=np.linalg.norm,N_args=[],cost=BinaryLogisticRegression.cost):\n",
    "        self.C=C\n",
    "        self.F=F\n",
    "        self.dF=dF\n",
    "        self.N=N\n",
    "        self.cost=cost\n",
    "    def fit(self,X,Y):\n",
    "        X=np.array(X)\n",
    "        Y=np.array(Y)\n",
    "        self.k=np.max(Y)+1\n",
    "        self.W=np.zeros([X.shape[1]+1,self.k])\n",
    "        for i in range(self.k):\n",
    "            self.W[:,i]=BinaryLogisticRegression(C=self.C,F=self.F,dF=self.dF,N=self.N).fit(X,Y==i).parameters()\n",
    "        \n",
    "    def predict(self,X):\n",
    "        S=np.zeros([X.shape[0],self.k])\n",
    "        V=self.F(np.array(X)@self.W[1:,:]+self.W[0,:])\n",
    "        \n",
    "        return np.argmax(V,axis=1)\n",
    "    def parameters():\n",
    "        return self.W\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def S0(x):\n",
    "    if(x<=0):\n",
    "        return 0\n",
    "    return np.exp(-1/x**2)\n",
    "classifier=MultinomialLogisticRegression(C=10,N=lambda u:np.linalg.norm(u,ord=2),cost=lambda X,Y,w,F,C,N:\n",
    "                                      C*np.linalg.norm(F(X@w[1:]+w[0])-Y)+N(w[1:]))\n",
    "classifier.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "print((classifier.predict(X_test)==Y_test).mean())\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.868421052631579"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogisticRegression(C=1000,max_iter=1000).fit(X_train,Y_train).score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9210526315789473"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVC(kernel=\"poly\",degree=2).fit(X_train,Y_train).score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
