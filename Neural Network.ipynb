{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                  5.1               3.5                1.4               0.2   \n",
       "1                  4.9               3.0                1.4               0.2   \n",
       "2                  4.7               3.2                1.3               0.2   \n",
       "3                  4.6               3.1                1.5               0.2   \n",
       "4                  5.0               3.6                1.4               0.2   \n",
       "..                 ...               ...                ...               ...   \n",
       "145                6.7               3.0                5.2               2.3   \n",
       "146                6.3               2.5                5.0               1.9   \n",
       "147                6.5               3.0                5.2               2.0   \n",
       "148                6.2               3.4                5.4               2.3   \n",
       "149                5.9               3.0                5.1               1.8   \n",
       "\n",
       "     target      label  \n",
       "0         0     setosa  \n",
       "1         0     setosa  \n",
       "2         0     setosa  \n",
       "3         0     setosa  \n",
       "4         0     setosa  \n",
       "..      ...        ...  \n",
       "145       2  virginica  \n",
       "146       2  virginica  \n",
       "147       2  virginica  \n",
       "148       2  virginica  \n",
       "149       2  virginica  \n",
       "\n",
       "[150 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "def createIRIS():\n",
    "    data=datasets.load_iris()\n",
    "    U=pd.DataFrame(data[\"data\"],columns=data[\"feature_names\"])\n",
    "    labels=pd.Series(data[\"target_names\"])\n",
    "    U[\"target\"]=data[\"target\"]\n",
    "    U[\"label\"]= [labels[i] for i in data[\"target\"]]\n",
    "    return U\n",
    "U=createIRIS()\n",
    "U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X=StandardScaler().fit_transform(U[U.columns[:-2]])\n",
    "Y=U[U.columns[-2]]\n",
    "X_train,X_test,Y_train,Y_test= train_test_split(X,Y,test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I Deriving Neural Network\n",
    "$\\DeclareMathOperator {\\Diag}{Diag}$\n",
    "## 1.Notations & Definitions\n",
    "### 1.1. Experience\n",
    "- We will denote by $\\mathbb{1}_i \\in \\mathbb{R}^i $ the vector of ones $\\forall i \\in \\mathbb{N}$ \n",
    "- $\\mathscr{A}\\left(E,F\\right)$ is the set of affine transformations between $E$ and $F$\n",
    "- Let $m=d_1$ the number of features.\n",
    "- Let $n \\in \\mathbb{N}^*$ the number of samples\n",
    "- Let $X \\in E^n$ be a tuple of input samples \n",
    "- Let $k=d_L$ the number of classes of the output\n",
    "- Let $y \\in \\{0,k-1\\}^n$ be a tuple of samples \n",
    "- Let $Y_i=(\\delta_{s,y_i})_{s\\in\\{1,\\dots,k\\}}\\forall i \\in \\{1,\\dots,n\\}$ the representation of each ouput on the neural network\n",
    "\n",
    "### 1.2. Neural network\n",
    "- Let $L\\in \\mathbb{N}_{\\ge 2}$ be the number of all layers\n",
    "- Let $d_1,\\dots,d_{L}$ be the dimension of the respective layer\n",
    "- Let $E_i=\\mathbb{R}^{d_i}$\n",
    "- Let $\\mathscr{A}_i=\\mathscr{A}\\left(E_{i-1},E_i\\right),i\\in\\left\\{2,\\dots,L\\right\\}$\n",
    "- Let $\\sigma_i \\in \\mathscr{C}^1\\left(E_i,E_i\\right) \\forall i \\in \\{2,\\dots,L\\}$ be a series of activation functions with $\\sigma_L$ a cumulative distribution function so that the final output can be interpreted as a probability\n",
    "- Let $\\mathscr{K}=\\prod_{i=2}^L\\mathscr{A}_i$ the vector space in which the parameters vary\n",
    "- Let $\\Psi\\in \\mathscr{C}^1\\left(\\mathscr{K}\\times E_1,E_L\\right)/$\n",
    "$$\\forall \\boldsymbol{T}=(T_2,\\dots,T_L),\\in\\mathscr{K},\\forall u \\in E_1,\\quad\\Psi(\\boldsymbol{T},u)=\\left(\\bigcirc_{i=0}^{L-2}\\sigma_{L-i}\\circ T_{L-i}\\right)(u)$$\n",
    "- Let $C \\in \\mathbb{R}_+$ be a hyperparameter\n",
    "- Let $\\mathscr{N}=\\lVert\\cdot\\rVert$ be a norm on $\\mathscr{K}$\n",
    "- Let $\\mathscr{l}\\in \\mathscr{F}\\left([0,1]^{k}\\times [0,1]^{k},\\mathbb{R}\\right)$ the unit cost function\n",
    "- Let $\\mathscr{L} \\in \\mathscr{F}\\left(\\mathscr{K},\\mathbb{R}\\right)$ the cost function for this experience:\n",
    "$$\\forall \\boldsymbol{T} \\in \\mathscr{K},\\quad \\mathscr{L}(\\boldsymbol{T})=C\\sum_{i=1}^{n}\\mathscr{l}\\left(\\Psi(\\boldsymbol{T},X_i),Y_i\\right)+\\lVert \\boldsymbol{T}\\rVert$$\n",
    "- For each vector $X_i,i \\in \\{1,\\dots,n\\}$, We will denote by $\\left(^{(i)}a^{(j)}\\right)_{j\\in\\{1,\\dots,L\\}}$ and $\\left(^{(i)}z^{(j)}\\right)_{j\\in\\{1,\\dots,L\\}}$ the following sequences:\n",
    "$$\\begin{cases}^{(i)}a^{(1)}= ^{(i)}z^{(1)}=X_i \\\\ \n",
    "^{(i)}a^{(j)} = \\sigma_i\\left(^{(i)}z^{(j)}\\right) \\\\\n",
    "^{(i)}z^{(j)} = T_i\\left(^{(i)}a^{(j-1)}\\right)\n",
    "\\end{cases}$$\n",
    "- We will use as a notation $\\dfrac{\\partial f}{\\partial u}\\left(u_0\\right)=J_f\\left(u_0\\right)$\n",
    "- We will denote a neural network any tuple $\\mathcal{N}=\\left(\\boldsymbol{E},\\boldsymbol{T},\\boldsymbol{\\sigma}\\right)$, where: $$\\begin{cases} \n",
    "\\boldsymbol{E}=\\left(E_1,\\dots,E_L\\right) \\\\\n",
    "\\boldsymbol{\\sigma}=\\left(\\sigma_2,\\dots,\\sigma_L\\right)\n",
    "\\end{cases}\n",
    "$$\n",
    "### 1.3. Diagram\n",
    "A neural network $\\mathcal{N}$ with $\\begin{cases}\n",
    "\\boldsymbol{E}=\\left(\\mathbb{R}^{12},\\mathbb{R}^6,\\mathbb{R}^6,\\mathbb{R}^2\\right) \\\\\n",
    "\\boldsymbol{T}=\\left(T_2,T_3,T_4\\right)\\\\\n",
    "\\boldsymbol{\\sigma}=\\left(\\sigma_2,\\sigma_3,\\sigma_4\\right)\n",
    "\\end{cases}\n",
    "$\n",
    "<p style=\"text-align: center;\">\n",
    "<img src=\"nn.png\" style=\"text-align: center;\">\n",
    "</p>\n",
    "\n",
    "## 2. Objective\n",
    "We will try to find: $$\\boldsymbol{T}_*=\\arg\\min_{\\boldsymbol{T}\\in \\mathscr{K}} \\mathscr{L}(\\boldsymbol{T})$$\n",
    "## 3. Strategy\n",
    "The problem maybe too hard to crack globally, So assuming that $\\sigma$ is a $\\mathscr{C}^1$ function, we will fix a $\\boldsymbol{T}_0\\in \\mathscr{K}$ and try to find a local minimum close to $\\boldsymbol{T}_0$\n",
    "## 4. Backtracking\n",
    "Let $i\\in\\{1,\\cdots,n\\},j\\in \\{2,\\cdots,L\\}$\n",
    "We have the following identities:\n",
    "1. $$ \\dfrac{\\partial a^{(j)}}{\\partial z^{(j)}} =  \\dfrac{\\partial \\sigma_j(z^{(j)})}{\\partial z^{(j)}} = \\dfrac{\\partial \\sigma_j}{\\partial z^{(j)}}$$\n",
    "2. $$ \\dfrac{\\partial z^{(j)}}{\\partial a^{(j-1)}}= \\dfrac{\\partial \\left(T_j(a^{(j-1)})\\right)}{\\partial a^{(j-1)}}=\\dfrac{\\partial \\left(f_j(a^{(j-1)})+b_j\\right)}{\\partial a^{(j-1)}}=\\dfrac{\\partial f_j}{\\partial a^{(j-1)}}=f_j \\quad \\text{where }\n",
    "\\begin{cases} f_j=T_j-b_j \\in \\mathscr{L}\\left(E_{j-1},E_j\\right) \\\\ b_j=T_j(0)\n",
    "\\end{cases}$$ \n",
    "3. $$ \\dfrac{\\partial z^{(j)}}{\\partial f_j}= \\dfrac{\\partial \\left(f_j(z^{(j-1)})+b_j\\right)}{\\partial f_j}=\\dfrac{\\partial \\left(f_j(a^{(j-1)})\\right)}{\\partial f_j} = \\begin{pmatrix}\n",
    "(a^{(j-1)})^T & \\cdots & \\cdots & \\boldsymbol{0}_{E_{j-1}}^T\\\\\n",
    "\\boldsymbol{0}_{E_{j-1}}^T &  (a^{(j-1)})^T & \\cdots & \\boldsymbol{0}_{E_{j-1}}^T\\\\\n",
    "\\vdots & \\vdots & \\ddots  & \\boldsymbol{0}_{E_{j-1}}^T\\\\\n",
    "\\boldsymbol{0}_{E_{j-1}}^T & \\cdots & \\cdots & (a^{(j-1)})^T\n",
    "\\end{pmatrix} $$\n",
    "4. $$ \\dfrac{\\partial z^{(j)}}{\\partial b_j}= \\dfrac{\\partial \\left(f_j(a^{(j-1)})+b_j\\right)}{\\partial b_j}= I_{d_j} $$\n",
    "5. $$\\dfrac{\\partial z^{(j)}}{\\partial T_j} = \n",
    "\\begin{pmatrix}\n",
    "\\dfrac{\\partial z^{(j)}}{\\partial f_j} & \\dfrac{\\partial z^{(j)}}{\\partial b_j}\n",
    "\\end{pmatrix}=\n",
    " \\begin{pmatrix}\n",
    "(a^{(j-1)})^T & \\cdots & \\cdots & \\boldsymbol{0}_{E_{j-1}}^T & 1 & 0 & \\cdots & 0\\\\\n",
    "\\boldsymbol{0}_{E_{j-1}}^T &  (a^{(j-1)})^T & \\cdots & \\boldsymbol{0}_{E_{j-1}}^T & 0 &1 & \\cdots & 0\\\\\n",
    "\\vdots & \\vdots & \\ddots  & \\boldsymbol{0}_{E_{j-1}}^T & \\vdots & \\vdots & \\ddots & \\vdots & \\\\\n",
    "\\boldsymbol{0}_{E_{j-1}}^T & \\cdots & \\cdots & (a^{(j-1)})^T & 0 & \\cdots & \\cdots & 1\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "## 5. Exemples of Activation Functions & Cost Function\n",
    "\n",
    "### 5.1.  Cost function:\n",
    "A widely used unit cost function is the cross-entropy loss function\n",
    "$$ \\mathscr{l}(u,v)= \\lVert-v\\odot \\ln(u)- (\\mathbb{1}-v)\\odot \\ln(\\mathbb{1}-u)\\rVert_1 = \\sum_{i=1}^{k}-v_i\\ln(u_i)- (1-v_i)\\ln(1-u_i)$$\n",
    "The cost function is then:\n",
    "$$ \\mathscr{L}(\\boldsymbol{T})=C\\sum_{i=1}^{n}\\mathscr{l}\\left(\\Psi(\\boldsymbol{T},X_i),Y_i\\right)+\\lVert \\boldsymbol{T}\\rVert = C\\sum_{i=1}^{n}\\sum_{j=1}^{k}-Y_{i,j}\\ln(\\Psi(\\boldsymbol{T},X_i))_j- (1-Y_{i,j})\\ln(1-\\Psi(\\boldsymbol{T},X_i))_j+\\lVert \\boldsymbol{T}\\rVert   $$\n",
    "### 5.2 Activation functions & their jacobians:\n",
    "Here raising a vector to an exponent is done element wise.\n",
    "1. Hyperbolic tangent $\\sigma_j=\\tanh$ applied element-wise:\n",
    "$$ \\dfrac{\\partial a^{(j)}}{\\partial z^{(j)}} = \\dfrac{\\partial \\sigma_j}{\\partial z^{(j)}}= \\Diag\\left(\\sigma_j'(z^{(j)})\\right)=\\Diag\\left(\\mathbb{1}-\\left(\\sigma(z^{(j)})\\odot\\sigma(z^{(j)})\\right)\\right)=\\Diag\\left(\\mathbb{1}-\\left(\\sigma(z^{(j)})^2\\right)\\right)=\\Diag\\left(\\mathbb{1}-\\left((a^{(j)})^2\\right)\\right)$$\n",
    "2. Logistic function $\\sigma_j=\\sigma$ applied element-wise: \n",
    "$$ \\dfrac{\\partial a^{(j)}}{\\partial z^{(j)}} = \\dfrac{\\partial \\sigma_j}{\\partial z^{(j)}}= \\Diag\\left(\\sigma_j'(z^{(j)})\\right)=\\Diag\\left(\\sigma(z^{(j)})\\odot\\left(\\mathbb{1}-\\sigma(z^{(j)})\\right) \\right)=\\Diag\\left(a^{(j)}\\odot\\left(\\mathbb{1}-a^{(j)}\\right) \\right)$$Â£\n",
    "\n",
    "3. Guassian function $\\sigma_j=g=e^{-x^2}$:\n",
    "$$ \\dfrac{\\partial a^{(j)}}{\\partial z^{(j)}} = \\dfrac{\\partial \\sigma_j}{\\partial z^{(j)}}= \\Diag\\left(g'(z^{(j)})\\right)=-2\\Diag\\left(z^{(j)}\\odot g\\left(z^{(j)}\\right)\\right)=-2\\Diag\\left(z^{(j)} \\odot a^{(j)} \\right)$$\n",
    "\n",
    "### 5.3 Cumulative Distribution functions:\n",
    "Note that every CDF can be used as an activation function\n",
    "1. Logistic function $\\sigma$\n",
    "\n",
    "# II. Implementing Neural Network\n",
    "For simplicity, the implemenatation will use only one activation function: $\\sigma$\n",
    "## 1. Creating Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "from scipy.special import xlogy\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "class ActivationFunction:\n",
    "    def __init__(self,f,df):\n",
    "        self.f=f\n",
    "        self.df=df\n",
    "    pass\n",
    "\n",
    "logistic_function=ActivationFunction(sigmoid,lambda x:sigmoid(x)*(1-sigmoid(x)))\n",
    "identity_function=ActivationFunction(lambda x:x,lambda x:1)\n",
    "guassian_function=ActivationFunction(lambda x:np.exp(-x**2),lambda x:-2*x*np.exp(-x**2))\n",
    "tanh_function=ActivationFunction(lambda x:np.tanh(x),lambda x:1-np.tanh(x)**2)\n",
    "ReLU_function=ActivationFunction(lambda x:np.maximum(x,0),lambda x:(x>0)*1)\n",
    "Softsign_function=ActivationFunction(lambda x:x/(1+np.abs(x)),lambda x:1/(1+np.abs(x))**2)\n",
    "\n",
    "class NeuralNetworkDesigner:\n",
    "    def __init__(self,x_dim,y_dim):\n",
    "        x_dim=int(x_dim)\n",
    "        y_dim=int(y_dim)\n",
    "        self.design=[x_dim]\n",
    "        self.activation_functions=[]\n",
    "        self.y_dim=y_dim\n",
    "\n",
    "#Adding hidden layer\n",
    "    def add_layer(self,dim,act=logistic_function):\n",
    "        self.design.append(dim)\n",
    "        self.activation_functions.append(act)\n",
    "        return self\n",
    "    \n",
    "#Adding final layer \n",
    "    def close_network(self,act=logistic_function):\n",
    "        self.design.append(self.y_dim)\n",
    "        self.activation_functions.append(act)\n",
    "    \n",
    "#Reshaping a vector according to the design\n",
    "    def reshape(self,U):\n",
    "        design=self.design\n",
    "        M=[]\n",
    "        k=0\n",
    "        b=[]\n",
    "        for i in range(len(design)-1):\n",
    "            M.append(np.reshape(U[k:k+design[i]*design[i+1]],[design[i+1],design[i]]))\n",
    "            k=k+design[i]*design[i+1]\n",
    "            b.append(U[k:k+design[i+1]])\n",
    "            k=k+design[i+1]            \n",
    "        return M,b\n",
    "\n",
    "#flattening according to the design    \n",
    "    def flatten(self,M,b):\n",
    "        design=self.design\n",
    "        S=np.array(design)\n",
    "        s=np.sum(S[:-1]*S[1:])\n",
    "        r=np.sum(S[1:])\n",
    "        U=np.zeros(s+r)\n",
    "        k=0\n",
    "        for i in range(len(design)-1):\n",
    "            U[k:k+design[i]*design[i+1]]=np.ndarray.flatten(M[i])\n",
    "            k=k+design[i]*design[i+1]\n",
    "            U[k:k+design[i+1]]=b[i]\n",
    "            k=k+design[i+1]\n",
    "        return U\n",
    "    \n",
    "    def flattened_dimension(self):\n",
    "        S=np.array(self.design)\n",
    "        return np.sum(S[:-1]*S[1:])+np.sum(S[1:])\n",
    "    def xdim(self):\n",
    "        return self.design[0]\n",
    "    def ydim(self):\n",
    "        return self.y_dim\n",
    "    pass\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, designer,C=1):\n",
    "        self.M=[]\n",
    "        self.designer=designer\n",
    "        self.design=self.designer.design\n",
    "        self.b=[]\n",
    "        self.C=C\n",
    "        self.L=len(self.design)\n",
    "        for i in range(self.L-1):\n",
    "            self.M.append(np.random.normal(0,1,[self.design[i+1],self.design[i]]))\n",
    "            self.b.append(np.random.normal(0,1,[self.design[i+1]]))\n",
    "    \n",
    "    def _predict_1(self,x):\n",
    "        a = self.decision_function_1(x)\n",
    "        return np.argmax(a,axis=0)\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.apply_along_axis(self._predict_1,arr=X.T,axis=0)\n",
    "    \n",
    "    def _unit_cost(self,u,w=None,retJac=False):\n",
    "        y=u[-1]\n",
    "        x=u[:-1]\n",
    "        M,b=self.designer.reshape(w)\n",
    "        a=[x]\n",
    "        z=[x]\n",
    "        act_functions=self.designer.activation_functions\n",
    "        for i in range(0,self.L-1):\n",
    "            z.append(M[i]@a[i]+b[i])\n",
    "            a.append(act_functions[i].f(z[-1]))\n",
    "        k=self.designer.ydim()\n",
    "        v=np.zeros(k)\n",
    "        v[int(y)]=1\n",
    "        v_predicted=a[-1]\n",
    "        l=-xlogy(v,v_predicted)-xlogy(1-v,1-v_predicted)\n",
    "        if not retJac:\n",
    "            return np.sum(l)\n",
    "        \n",
    "        chain = (-v/a[-1]+(1-v)/(1-a[-1]))\n",
    "        k=self.designer.flattened_dimension()\n",
    "        jac=np.zeros([1,k])\n",
    "        design=self.designer.design\n",
    "        for i in range(self.L-1,0,-1):\n",
    "            chain=chain@np.diag(act_functions[i-1].df(z[i]))\n",
    "            k=k-design[i]\n",
    "            jac[0,k:k+design[i]]=chain\n",
    "            k=k-design[i]*design[i-1]        \n",
    "            J=np.zeros([design[i],design[i]*design[i-1]])\n",
    "            for j in range(0,design[i]):\n",
    "                J[j,j*design[i-1] : (j+1)*design[i-1]]=a[i-1].T\n",
    "            jac[0,k:k+design[i-1]*design[i]]=chain@J\n",
    "            chain=chain@M[i-1]\n",
    "        return np.sum(l),jac\n",
    "    \n",
    "    def cost(self,X,Y,w=None,retJac=False):\n",
    "        if w is None:\n",
    "            w=nn.flattened_parameters()\n",
    "        S=self.designer.ydim()*X.shape[0]\n",
    "        R=np.apply_along_axis(lambda u,w:self._unit_cost(u,w,retJac=retJac),arr=np.c_[X,Y],axis=1,w=w)\n",
    "        L2=np.linalg.norm(w)\n",
    "        if not retJac:\n",
    "            return (self.C*np.sum(R)+L2)/S\n",
    "        L=self.C*np.sum(R[:,0])+L2\n",
    "        jac=np.ndarray.flatten(self.C*np.sum(R[:,1],axis=-1)+w.T/L2)\n",
    "        return L/S,jac/S\n",
    "    \n",
    "    def fit(self,X,Y):\n",
    "        self.M,self.b=self.designer.reshape(minimize(lambda w:self.cost(X,Y,w,retJac=True),\n",
    "                                      x0= self.designer.flatten(self.M,self.b),jac=True).x)\n",
    "        return self\n",
    "    \n",
    "    def decision_function_1(self,x):\n",
    "        y=x\n",
    "        for i in range(0,self.L-1):\n",
    "            y=self.designer.activation_functions[i].f(self.M[i]@y+self.b[i])\n",
    "        return y\n",
    "    \n",
    "    def decision_function(self,X):\n",
    "        return np.apply_along_axis(self.decision_function_1,arr=X,axis=1)\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.M,self.b\n",
    "    \n",
    "    def flattened_parameters(self):\n",
    "        return self.designer.flatten(self.M,self.b)\n",
    "    \n",
    "    def score(self,X,Y):\n",
    "        return (self.predict(X)==Y).mean()\n",
    "    pass\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "designer = NeuralNetworkDesigner(X.shape[1],3)\n",
    "designer.add_layer(6,ReLU_function)\n",
    "designer.close_network()\n",
    "nn=NeuralNetwork(designer,C=1)\n",
    "T=nn.flattened_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Verifying Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rami Zouari\\AppData\\Roaming\\Python\\Python37\\site-packages\\numpy\\core\\_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean distance between given gradient and the approximated gradient using finite difference:\n",
      "8.964186204174731e-08\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import check_grad\n",
    "checker= check_grad(lambda T: nn.cost(X,Y,T),x0=designer.flatten(nn.M,nn.b),grad=lambda T: nn.cost(X,Y,T,retJac=True)[1])\n",
    "print(\"Euclidean distance between given gradient and the approximated gradient using finite difference:\\n{}\".format(checker))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost Before training: 2.111293556279112\n",
      "Gradient Norm Before training: 1.5139691177745063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:112: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:112: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\Users\\Rami Zouari\\AppData\\Roaming\\Python\\Python37\\site-packages\\numpy\\core\\_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost After training: 0.17695695897180033\n",
      "Gradient Norm After training: 0.1036462915765578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rami Zouari\\AppData\\Roaming\\Python\\Python37\\site-packages\\numpy\\core\\_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    }
   ],
   "source": [
    "oldCost,oldJac=nn.cost(X_train,Y_train,retJac=True)\n",
    "print(\"Cost Before training: {}\".format(oldCost))\n",
    "print(\"Gradient Norm Before training: {}\".format(np.linalg.norm(oldJac)))\n",
    "nn.fit(X_train,Y_train)\n",
    "T=nn.flattened_parameters()\n",
    "\n",
    "newCost,newJac=nn.cost(X_train,Y_train,retJac=True)\n",
    "print(\"Cost After training: {}\".format(newCost))\n",
    "print(\"Gradient Norm After training: {}\".format(np.linalg.norm(newJac)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Accuarcy\n",
      "Training Accuarcy 0.95\n",
      "Testing Accuarcy 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking Accuarcy\")\n",
    "print(\"Training Accuarcy {}\".format(nn.score(X_train,Y_train)))\n",
    "print(\"Testing Accuarcy {}\".format(nn.score(X_test,Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Confusion Matrix of the model')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "sns.heatmap(confusion_matrix(Y_test,nn.predict(X_test)),annot=True)\n",
    "plt.title(\"Confusion Matrix of the model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.92588331, -1.02842996, -2.33229536, -2.89777983,  0.04289147,\n",
       "       -0.09098926,  0.39287331, -0.20872151, -1.20234379,  2.66609593,\n",
       "       -1.78599054, -1.99231355,  2.06827882,  2.81789673,  1.7768961 ,\n",
       "        3.50873006,  0.17698807, -1.02525139,  0.23878492,  1.16739991,\n",
       "       -0.11025648, -0.13394498, -1.1577895 , -3.567073  ,  0.67539112,\n",
       "       -0.62420888, -0.58972541,  0.63374857,  0.29620945,  1.23788528,\n",
       "       -0.57841844,  1.05016507,  2.31327301, -3.00192957, -1.62150764,\n",
       "        1.59461379,  0.21331018,  0.8688978 , -2.34118657, -0.33630138,\n",
       "       -0.99430972, -1.13526412, -1.01706369,  1.45932833, -2.38565627,\n",
       "        0.38645542,  0.98652209, -1.5631535 , -3.64875574,  3.36049826,\n",
       "       -3.14042706])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[254.33333 , 254.      , 252.      , ...,  87.      ,  88.666664,\n",
       "         87.      ],\n",
       "       [ 39.      ,  50.666668,  47.      , ..., 117.666664, 115.      ,\n",
       "        133.      ],\n",
       "       [ 89.666664, 103.666664, 126.333336, ..., 175.33333 , 183.33333 ,\n",
       "        182.66667 ],\n",
       "       ...,\n",
       "       [ 86.666664,  80.      ,  74.333336, ...,  44.333332,  50.      ,\n",
       "         44.666668],\n",
       "       [ 50.666668,  65.333336,  88.333336, ..., 196.66667 , 178.66667 ,\n",
       "        165.66667 ],\n",
       "       [ 30.      ,  27.      ,  33.      , ...,  35.      ,  35.666668,\n",
       "         61.      ]], dtype=float32)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 6, 3, ..., 5, 3, 5], dtype=int64)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline([(\"scaler\",StandardScaler()),(\"reduction\",PCA(n_components=27))])\n",
    "X1=pipe.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-0c589a604d5b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X1' is not defined"
     ]
    }
   ],
   "source": [
    "X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import (SVC,LinearSVC)\n",
    "SVC(kernel=\"rbf\",gamma=.2).fit(X_train,Y_train).score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
