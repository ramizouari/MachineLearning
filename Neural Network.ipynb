{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "def createIRIS():\n",
    "    data=datasets.load_iris()\n",
    "    U=pd.DataFrame(data[\"data\"],columns=data[\"feature_names\"])\n",
    "    labels=pd.Series(data[\"target_names\"])\n",
    "    U[\"target\"]=data[\"target\"]\n",
    "    U[\"label\"]= [labels[i] for i in data[\"target\"]]\n",
    "    return U\n",
    "\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
    "\n",
    "# introspect the images arrays to find the shapes (for plotting)\n",
    "n_samples, h, w = lfw_people.images.shape\n",
    "\n",
    "# for machine learning we use the 2 data directly (as relative pixel\n",
    "# positions info is ignored by this model)\n",
    "X = lfw_people.data\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# the label to predict is the id of the person\n",
    "y = lfw_people.target\n",
    "target_names = lfw_people.target_names\n",
    "n_classes = target_names.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "data3 = pd.read_csv(\"../Learning Test/heart_failure.csv\")\n",
    "data3.sample(10)\n",
    "X=StandardScaler().fit_transform(data3[data3.columns[:-1]])\n",
    "Y=data3[data3.columns[-1]]\n",
    "X_train,X_test,Y_train,Y_test= train_test_split(X,Y,test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I Deriving Neural Network\n",
    "$\\DeclareMathOperator {\\Diag}{Diag}$\n",
    "## 1.Notations & Definitions\n",
    "### a. Experience\n",
    "- We will denote by $\\mathbb{1}_i \\in \\mathbb{R}^i $ the vector of ones $\\forall i \\in \\mathbb{N}$ \n",
    "- $\\mathscr{A}\\left(E,F\\right)$ is the set of affine transformations between $E$ and $F$\n",
    "- Let $m=d_1$ the number of features.\n",
    "- Let $n \\in \\mathbb{N}^*$ the number of samples\n",
    "- Let $X \\in E^n$ be a tuple of input samples \n",
    "- Let $k=d_L$ the number of classes of the output\n",
    "- Let $y \\in \\{0,k-1\\}^n$ be a tuple of samples \n",
    "- Let $Y_i=(\\delta_{s,y_i})_{s\\in\\{1,\\dots,k\\}}\\forall i \\in \\{1,\\dots,n\\}$ the representation of each ouput on the neural network\n",
    "### b. Neural network\n",
    "- Let $L\\in \\mathbb{N}_{\\ge 2}$ be the number of all layers\n",
    "- Let $d_1,\\dots,d_{L}$ be the dimension of the respective layer\n",
    "- Let $E_i=\\mathbb{R}^{d_i}$\n",
    "- Let $\\mathscr{A}_i=\\mathscr{A}\\left(E_{i-1},E_i\\right),i\\in\\left\\{2,\\dots,L\\right\\}$\n",
    "- Let $\\phi_i \\in \\mathscr{C}^1\\left(E_i,E_i\\right) \\forall i \\in \\{2,\\dots,L\\}$ be a series of activation functions with $\\phi_L$ a cumulative distribution function so that the final output can be interpreted as a probability\n",
    "- Let $\\mathscr{K}=\\prod_{i=2}^L\\mathscr{A}_i$ the vector space in which the parameters vary\n",
    "- Let $\\Psi\\in \\mathscr{C}^1\\left(\\mathscr{K}\\times E_1,E_L\\right)/$\n",
    "$$\\forall \\boldsymbol{T}=(T_2,\\dots,T_L),\\in\\mathscr{K},\\forall u \\in E_1,\\quad\\Psi(\\boldsymbol{T},u)=\\left(\\bigcirc_{i=0}^{L-2}\\phi_{L-i}\\circ T_{L-i}\\right)(u)$$\n",
    "- Let $C \\in \\mathbb{R}_+$ be a hyperparameter\n",
    "- Let $\\mathscr{N}=\\lVert\\cdot\\rVert$ be a norm on $\\mathscr{K}$\n",
    "- Let $\\mathscr{l}\\in \\mathscr{F}\\left([0,1]^{k}\\times [0,1]^{k},\\mathbb{R}\\right)$ the unit cost function\n",
    "- Let $\\mathscr{L} \\in \\mathscr{F}\\left(\\mathscr{K},\\mathbb{R}\\right)$ the cost function for this experience:\n",
    "$$\\forall \\boldsymbol{T} \\in \\mathscr{K},\\quad \\mathscr{L}(\\boldsymbol{T})=C\\sum_{i=1}^{n}\\mathscr{l}\\left(\\Psi(\\boldsymbol{T},X_i),Y_i\\right)+\\lVert \\boldsymbol{T}\\rVert$$\n",
    "- For each vector $X_i,i \\in \\{1,\\dots,n\\}$, We will denote by $\\left(^{(i)}a^{(j)}\\right)_{j\\in\\{1,\\dots,L\\}}$ and $\\left(^{(i)}z^{(j)}\\right)_{j\\in\\{1,\\dots,L\\}}$ the following sequences:\n",
    "$$\\begin{cases}^{(i)}a^{(1)}= ^{(i)}z^{(1)}=X_i \\\\ \n",
    "^{(i)}a^{(j)} = \\phi_i\\left(^{(i)}z^{(j)}\\right) \\\\\n",
    "^{(i)}z^{(j)} = T_i\\left(^{(i)}a^{(j-1)}\\right)\n",
    "\\end{cases}$$\n",
    "- We will use as a notation $\\dfrac{\\partial f}{\\partial u}\\left(u_0\\right)=J_f\\left(u_0\\right)$\n",
    "- We will denote a neural network any tuple $\\mathcal{N}=\\left(\\boldsymbol{E},\\boldsymbol{T},\\boldsymbol{\\phi}\\right)$, where: $$\\begin{cases} \n",
    "\\boldsymbol{E}=\\left(E_1,\\dots,E_L\\right) \\\\\n",
    "\\boldsymbol{\\phi}=\\left(\\phi_2,\\dots,\\phi_L\\right)\n",
    "\\end{cases}\n",
    "$$\n",
    "## 2. Objective\n",
    "We will try to find: $$\\boldsymbol{T}_*=\\arg\\min_{\\boldsymbol{T}\\in \\mathscr{K}} \\mathscr{L}(\\boldsymbol{T})$$\n",
    "## 3. Strategy\n",
    "The problem maybe too hard to crack globally, So assuming that $\\Phi$ is a $\\mathscr{C}^1$ function, we will fix a $\\boldsymbol{T}_0\\in \\mathscr{K}$ and try to find a local minimum close to $\\boldsymbol{T}_0$\n",
    "## 4. Backtracking\n",
    "Let $i\\in\\{1,\\cdots,n\\},j\\in \\{2,\\cdots,L\\}$\n",
    "We have the following identities:\n",
    "1. $$ \\dfrac{\\partial a^{(j)}}{\\partial z^{(j)}} =  \\dfrac{\\partial \\phi_j(z^{(j)})}{\\partial z^{(j)}} = \\dfrac{\\partial \\phi_j}{\\partial z^{(j)}}$$\n",
    "2. $$ \\dfrac{\\partial z^{(j)}}{\\partial a^{(j-1)}}= \\dfrac{\\partial \\left(T_j(a^{(j-1)})\\right)}{\\partial a^{(j-1)}}=\\dfrac{\\partial \\left(f_j(a^{(j-1)})+b_j\\right)}{\\partial a^{(j-1)}}=\\dfrac{\\partial f_j}{\\partial a^{(j-1)}}=f_j \\quad \\text{where }\n",
    "\\begin{cases} f_j=T_j-b_j \\in \\mathscr{L}\\left(E_{j-1},E_j\\right) \\\\ b_j=T_j(0)\n",
    "\\end{cases}$$ \n",
    "3. $$ \\dfrac{\\partial z^{(j)}}{\\partial f_j}= \\dfrac{\\partial \\left(f_j(z^{(j-1)})+b_j\\right)}{\\partial f_j}=\\dfrac{\\partial \\left(f_j(a^{(j-1)})\\right)}{\\partial f_j} = \\begin{pmatrix}\n",
    "(a^{(j-1)})^T & \\cdots & \\cdots & \\boldsymbol{0}_{E_{j-1}}^T\\\\\n",
    "\\boldsymbol{0}_{E_{j-1}}^T &  (a^{(j-1)})^T & \\cdots & \\boldsymbol{0}_{E_{j-1}}^T\\\\\n",
    "\\vdots & \\vdots & \\ddots  & \\boldsymbol{0}_{E_{j-1}}^T\\\\\n",
    "\\boldsymbol{0}_{E_{j-1}}^T & \\cdots & \\cdots & (a^{(j-1)})^T\n",
    "\\end{pmatrix} $$\n",
    "4. $$ \\dfrac{\\partial z^{(j)}}{\\partial b_j}= \\dfrac{\\partial \\left(f_j(a^{(j-1)})+b_j\\right)}{\\partial b_j}= I_{d_j} $$\n",
    "5. $$\\dfrac{\\partial z^{(j)}}{\\partial T_j} = \n",
    "\\begin{pmatrix}\n",
    "\\dfrac{\\partial z^{(j)}}{\\partial f_j} & \\dfrac{\\partial z^{(j)}}{\\partial b_j}\n",
    "\\end{pmatrix}=\n",
    " \\begin{pmatrix}\n",
    "(a^{(j-1)})^T & \\cdots & \\cdots & \\boldsymbol{0}_{E_{j-1}}^T & 1 & 0 & \\cdots & 0\\\\\n",
    "\\boldsymbol{0}_{E_{j-1}}^T &  (a^{(j-1)})^T & \\cdots & \\boldsymbol{0}_{E_{j-1}}^T & 0 &1 & \\cdots & 0\\\\\n",
    "\\vdots & \\vdots & \\ddots  & \\boldsymbol{0}_{E_{j-1}}^T & \\vdots & \\vdots & \\ddots & \\vdots & \\\\\n",
    "\\boldsymbol{0}_{E_{j-1}}^T & \\cdots & \\cdots & (a^{(j-1)})^T & 0 & \\cdots & \\cdots & 1\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "## 5. Exemples of Activation Functions & Cost Function\n",
    "\n",
    "### 5.1.  Cost function:\n",
    "A widely used unit cost function is the cross-entropy loss function\n",
    "$$ \\mathscr{l}(u,v)= \\lVert-v\\odot \\ln(u)- (\\mathbb{1}-v)\\odot \\ln(\\mathbb{1}-u)\\rVert_1 = \\sum_{i=1}^{k}-v_i\\ln(u_i)- (1-v_i)\\ln(1-u_i)$$\n",
    "The cost function is then:\n",
    "$$ \\mathscr{L}(\\boldsymbol{T})=C\\sum_{i=1}^{n}\\mathscr{l}\\left(\\Psi(\\boldsymbol{T},X_i),Y_i\\right)+\\lVert \\boldsymbol{T}\\rVert = C\\sum_{i=1}^{n}\\sum_{j=1}^{k}-Y_{i,j}\\ln(\\Psi(\\boldsymbol{T},X_i))_j- (1-Y_{i,j})\\ln(1-\\Psi(\\boldsymbol{T},X_i))_j+\\lVert \\boldsymbol{T}\\rVert   $$\n",
    "### 5.2 Activation functions & their jacobians:\n",
    "Here raising a vector to an exponent is done element wise.\n",
    "1. Hyperbolic tangent $\\phi_j=\\tanh$ applied element-wise:\n",
    "$$ \\dfrac{\\partial a^{(j)}}{\\partial z^{(j)}} = \\dfrac{\\partial \\phi_j}{\\partial z^{(j)}}= \\Diag\\left(\\phi_j'(z^{(j)})\\right)=\\Diag\\left(\\mathbb{1}-\\left(\\phi(z^{(j)})\\odot\\phi(z^{(j)})\\right)\\right)=\\Diag\\left(\\mathbb{1}-\\left(\\phi(z^{(j)})^2\\right)\\right)=\\Diag\\left(\\mathbb{1}-\\left((a^{(j)})^2\\right)\\right)$$\n",
    "2. Logistic function $\\phi_j=\\sigma$ applied element-wise: \n",
    "$$ \\dfrac{\\partial a^{(j)}}{\\partial z^{(j)}} = \\dfrac{\\partial \\phi_j}{\\partial z^{(j)}}= \\Diag\\left(\\phi_j'(z^{(j)})\\right)=\\Diag\\left(\\sigma(z^{(j)})\\odot\\left(\\mathbb{1}-\\sigma(z^{(j)})\\right) \\right)=\\Diag\\left(a^{(j)}\\odot\\left(\\mathbb{1}-a^{(j)}\\right) \\right)$$Â£\n",
    "\n",
    "3. Guassian function $\\phi_j=g=e^{-x^2}$:\n",
    "$$ \\dfrac{\\partial a^{(j)}}{\\partial z^{(j)}} = \\dfrac{\\partial \\phi_j}{\\partial z^{(j)}}= \\Diag\\left(g'(z^{(j)})\\right)=-2\\Diag\\left(z^{(j)}\\odot g\\left(z^{(j)}\\right)\\right)=-2\\Diag\\left(z^{(j)} \\odot a^{(j)} \\right)$$\n",
    "\n",
    "### 5.3 Cumulative Distribution functions:\n",
    "Note that every CDF can be used as an activation function\n",
    "1. Logistic function $\\sigma$\n",
    "\n",
    "# II. Implementing Neural Network\n",
    "For simplicity, the implemenatation will use only one activation function: $\\sigma$\n",
    "## 1. Creating Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "from scipy.special import xlogy\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "class ActivationFunction:\n",
    "    def __init__(self,f,df):\n",
    "        self.f=f\n",
    "        self.df=df\n",
    "    pass\n",
    "\n",
    "logistic_function=ActivationFunction(sigmoid,lambda x:sigmoid(x)*(1-sigmoid(x)))\n",
    "identity_function=ActivationFunction(lambda x:x,lambda x:1)\n",
    "guassian_function=ActivationFunction(lambda x:np.exp(-x**2),lambda x:-2*x*np.exp(-x**2))\n",
    "tanh_function=ActivationFunction(lambda x:np.tanh(x),lambda x:1-np.tanh(x)**2)\n",
    "ReLU_function=ActivationFunction(lambda x:np.maximum(x,0),lambda x:(x>0)*1)\n",
    "Softsign_function=ActivationFunction(lambda x:x/(1+np.abs(x)),lambda x:1/(1+np.abs(x))**2)\n",
    "\n",
    "class NeuralNetworkDesigner:\n",
    "    def __init__(self,x_dim,y_dim):\n",
    "        x_dim=int(x_dim)\n",
    "        y_dim=int(y_dim)\n",
    "        self.design=[x_dim]\n",
    "        self.activation_functions=[]\n",
    "        self.y_dim=y_dim\n",
    "\n",
    "#Adding hidden layer\n",
    "    def add_layer(self,dim,act=logistic_function):\n",
    "        self.design.append(dim)\n",
    "        self.activation_functions.append(act)\n",
    "        return self\n",
    "    \n",
    "#Adding final layer \n",
    "    def close_network(self,act=logistic_function):\n",
    "        self.design.append(self.y_dim)\n",
    "        self.activation_functions.append(act)\n",
    "    \n",
    "#Reshaping a vector according to the design\n",
    "    def reshape(self,U):\n",
    "        design=self.design\n",
    "        M=[]\n",
    "        k=0\n",
    "        b=[]\n",
    "        for i in range(len(design)-1):\n",
    "            M.append(np.reshape(U[k:k+design[i]*design[i+1]],[design[i+1],design[i]]))\n",
    "            k=k+design[i]*design[i+1]\n",
    "            b.append(U[k:k+design[i+1]])\n",
    "            k=k+design[i+1]            \n",
    "        return M,b\n",
    "\n",
    "#flattening according to the design    \n",
    "    def flatten(self,M,b):\n",
    "        design=self.design\n",
    "        S=np.array(design)\n",
    "        s=np.sum(S[:-1]*S[1:])\n",
    "        r=np.sum(S[1:])\n",
    "        U=np.zeros(s+r)\n",
    "        k=0\n",
    "        for i in range(len(design)-1):\n",
    "            U[k:k+design[i]*design[i+1]]=np.ndarray.flatten(M[i])\n",
    "            k=k+design[i]*design[i+1]\n",
    "            U[k:k+design[i+1]]=b[i]\n",
    "            k=k+design[i+1]\n",
    "        return U\n",
    "    \n",
    "    def flattened_dimension(self):\n",
    "        S=np.array(self.design)\n",
    "        return np.sum(S[:-1]*S[1:])+np.sum(S[1:])\n",
    "    def xdim(self):\n",
    "        return self.design[0]\n",
    "    def ydim(self):\n",
    "        return self.y_dim\n",
    "    pass\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, designer,C=1):\n",
    "        self.M=[]\n",
    "        self.designer=designer\n",
    "        self.design=self.designer.design\n",
    "        self.b=[]\n",
    "        self.C=C\n",
    "        self.L=len(self.design)\n",
    "        for i in range(self.L-1):\n",
    "            self.M.append(np.random.normal(0,1,[self.design[i+1],self.design[i]]))\n",
    "            self.b.append(np.random.normal(0,1,[self.design[i+1]]))\n",
    "    \n",
    "    def _predict_1(self,x):\n",
    "        a = self.decision_function_1(x)\n",
    "        return np.argmax(a,axis=0)\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.apply_along_axis(self._predict_1,arr=X.T,axis=0)\n",
    "    \n",
    "    def _unit_cost(self,u,w=None,retJac=False):\n",
    "        y=u[-1]\n",
    "        x=u[:-1]\n",
    "        M,b=self.designer.reshape(w)\n",
    "        a=[x]\n",
    "        z=[x]\n",
    "        act_functions=self.designer.activation_functions\n",
    "        for i in range(0,self.L-1):\n",
    "            z.append(M[i]@a[i]+b[i])\n",
    "            a.append(act_functions[i].f(z[-1]))\n",
    "        k=self.designer.ydim()\n",
    "        v=np.zeros(k)\n",
    "        v[int(y)]=1\n",
    "        v_predicted=a[-1]\n",
    "        l=-xlogy(v,v_predicted)-xlogy(1-v,1-v_predicted)\n",
    "        if not retJac:\n",
    "            return np.sum(l)\n",
    "        \n",
    "        chain = (-v/a[-1]+(1-v)/(1-a[-1]))\n",
    "        k=self.designer.flattened_dimension()\n",
    "        jac=np.zeros([1,k])\n",
    "        design=self.designer.design\n",
    "        for i in range(self.L-1,0,-1):\n",
    "            chain=chain@np.diag(act_functions[i-1].df(z[i]))\n",
    "            k=k-design[i]\n",
    "            jac[0,k:k+design[i]]=chain\n",
    "            k=k-design[i]*design[i-1]        \n",
    "            J=np.zeros([design[i],design[i]*design[i-1]])\n",
    "            for j in range(0,design[i]):\n",
    "                J[j,j*design[i-1] : (j+1)*design[i-1]]=a[i-1].T\n",
    "            jac[0,k:k+design[i-1]*design[i]]=chain@J\n",
    "            chain=chain@M[i-1]\n",
    "        return np.sum(l),jac\n",
    "    \n",
    "    def cost(self,X,Y,w=None,retJac=False):\n",
    "        if w is None:\n",
    "            w=nn.flattened_parameters()\n",
    "        S=self.designer.ydim()*X.shape[0]\n",
    "        R=np.apply_along_axis(lambda u,w:self._unit_cost(u,w,retJac=retJac),arr=np.c_[X,Y],axis=1,w=w)\n",
    "        L2=np.linalg.norm(w)\n",
    "        if not retJac:\n",
    "            return (self.C*np.sum(R)+L2)/S\n",
    "        L=self.C*np.sum(R[:,0])+L2\n",
    "        jac=np.ndarray.flatten(self.C*np.sum(R[:,1],axis=-1)+w.T/L2)\n",
    "        return L/S,jac/S\n",
    "    \n",
    "    def fit(self,X,Y):\n",
    "        self.M,self.b=self.designer.reshape(minimize(lambda w:self.cost(X,Y,w,retJac=True),\n",
    "                                      x0= self.designer.flatten(self.M,self.b),jac=True).x)\n",
    "        return self\n",
    "    \n",
    "    def decision_function_1(self,x):\n",
    "        y=x\n",
    "        for i in range(0,self.L-1):\n",
    "            y=self.designer.activation_functions[i].f(self.M[i]@y+self.b[i])\n",
    "        return y\n",
    "    \n",
    "    def decision_function(self,X):\n",
    "        return np.apply_along_axis(self.decision_function_1,arr=X,axis=1)\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.M,self.b\n",
    "    \n",
    "    def flattened_parameters(self):\n",
    "        return self.designer.flatten(self.M,self.b)\n",
    "    \n",
    "    def score(self,X,Y):\n",
    "        return (self.predict(X)==Y).mean()\n",
    "    pass\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "designer = NeuralNetworkDesigner(X.shape[1],2)\n",
    "designer.add_layer(4,Softsign_function)\n",
    "designer.close_network()\n",
    "nn=NeuralNetwork(designer,C=1)\n",
    "T=nn.flattened_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Verifying Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rami Zouari\\AppData\\Roaming\\Python\\Python37\\site-packages\\numpy\\core\\_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean distance between given gradient and the approximated gradient using finite difference:\n",
      "8.531313485259519e-08\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import check_grad\n",
    "checker= check_grad(lambda T: nn.cost(X,Y,T),x0=designer.flatten(nn.M,nn.b),grad=lambda T: nn.cost(X,Y,T,retJac=True)[1])\n",
    "print(\"Euclidean distance between given gradient and the approximated gradient using finite difference:\\n{}\".format(checker))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost Before training: 1.2362840220079174\n",
      "Gradient Norm Before training: 0.370055695670829\n",
      "Cost After training: 0.24124689632864113\n",
      "Gradient Norm After training: 3.6016216071157794e-05\n"
     ]
    }
   ],
   "source": [
    "oldCost,oldJac=nn.cost(X_train,Y_train,retJac=True)\n",
    "print(\"Cost Before training: {}\".format(oldCost))\n",
    "print(\"Gradient Norm Before training: {}\".format(np.linalg.norm(oldJac)))\n",
    "nn.fit(X_train,Y_train)\n",
    "T=nn.flattened_parameters()\n",
    "\n",
    "newCost,newJac=nn.cost(X_train,Y_train,retJac=True)\n",
    "print(\"Cost After training: {}\".format(newCost))\n",
    "print(\"Gradient Norm After training: {}\".format(np.linalg.norm(newJac)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Accuarcy\n",
      "Training Accuarcy 0.899581589958159\n",
      "Testing Accuarcy 0.75\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking Accuarcy\")\n",
    "print(\"Training Accuarcy {}\".format(nn.score(X_train,Y_train)))\n",
    "print(\"Testing Accuarcy {}\".format(nn.score(X_test,Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Confusion Matrix of the model')"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "sns.heatmap(confusion_matrix(Y_test,nn.predict(X_test)),annot=True)\n",
    "plt.title(\"Confusion Matrix of the model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.17200869,  3.11232378, -0.45802694,  1.05282859, -4.45476861,\n",
       "        2.0372838 ,  0.03368292, -4.20076224,  3.86447848,  0.07544178,\n",
       "       -1.49036502, -1.39150779,  4.13778607, -0.36321906, -2.18578804,\n",
       "       -2.05963823, -0.8922495 , -0.54592468, -0.4797148 ,  1.83636456,\n",
       "       -2.9312274 , -0.19290122,  0.18471451, -5.26957774,  1.95666085,\n",
       "        0.41708331, -0.38802881,  0.6930646 , -1.7838974 , -2.38827759,\n",
       "        2.79841561,  4.52069395,  1.09817345,  0.93894753, -1.31121773,\n",
       "       -5.03404712,  1.46853178,  2.28488582, -3.19875111, -0.6979512 ,\n",
       "       -1.13473024, -0.55120783,  2.43311562, -4.7289156 ,  2.76216201,\n",
       "        1.64761146, -0.99676963,  0.10303871, -1.53112036, -4.67790006,\n",
       "       -2.15248378,  2.68155062, -2.81097492, -2.24331968, -1.943958  ,\n",
       "        3.25150868,  2.81292193,  2.24503672,  1.94522774, -3.25422169,\n",
       "       -0.97212565,  0.97367288])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[254.33333 , 254.      , 252.      , ...,  87.      ,  88.666664,\n",
       "         87.      ],\n",
       "       [ 39.      ,  50.666668,  47.      , ..., 117.666664, 115.      ,\n",
       "        133.      ],\n",
       "       [ 89.666664, 103.666664, 126.333336, ..., 175.33333 , 183.33333 ,\n",
       "        182.66667 ],\n",
       "       ...,\n",
       "       [ 86.666664,  80.      ,  74.333336, ...,  44.333332,  50.      ,\n",
       "         44.666668],\n",
       "       [ 50.666668,  65.333336,  88.333336, ..., 196.66667 , 178.66667 ,\n",
       "        165.66667 ],\n",
       "       [ 30.      ,  27.      ,  33.      , ...,  35.      ,  35.666668,\n",
       "         61.      ]], dtype=float32)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 6, 3, ..., 5, 3, 5], dtype=int64)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline([(\"scaler\",StandardScaler()),(\"reduction\",PCA(n_components=27))])\n",
    "X1=pipe.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3.0172794 ,   6.49042   ,   3.1736598 , ...,   1.9343585 ,\n",
       "          0.60026014,  -0.5922021 ],\n",
       "       [ 13.77223   , -12.467867  , -10.921871  , ...,   1.8391843 ,\n",
       "          0.44194543,  -1.5085399 ],\n",
       "       [-13.771272  ,  -5.349074  ,   1.0035915 , ...,  -1.5860572 ,\n",
       "          2.0146139 ,   3.4282625 ],\n",
       "       ...,\n",
       "       [ 36.41226   ,  10.861897  ,  -4.7760625 , ...,  -0.42443255,\n",
       "         -0.03802991,  -4.4505973 ],\n",
       "       [-34.95065   , -10.9131155 ,  13.111786  , ...,  -1.3024333 ,\n",
       "          0.3175527 ,   0.23040141],\n",
       "       [ 12.020468  ,  -4.7087    ,  10.997842  , ...,   4.358476  ,\n",
       "         -1.857329  ,   1.6596729 ]], dtype=float32)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7298136645962733"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_designer=NeuralNetworkDesigner(X1.shape[1],n_classes)\n",
    "nn_designer.add_layer(16).add_layer(10)\n",
    "nn_designer.close_network()\n",
    "nn_eigen=NeuralNetwork(nn_designer)\n",
    "X1_train,X1_test,Y1_train,Y1_test=train_test_split(X1,Y)\n",
    "from sklearn.svm import (SVC,LinearSVC)\n",
    "SVC(kernel=\"rbf\",gamma=.001).fit(X1_train,Y1_train).score(X1_test,Y1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 6, 3, ..., 5, 3, 5], dtype=int64)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
