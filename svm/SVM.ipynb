{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Support Vector Machine\n",
    "# I Deriving SVM\n",
    "We will begin with the binary classification problem, which can then be trivially generalized to multi-class problems using OneVsOne or OneVsAll\n",
    "## 1.Notations & Definitions\n",
    "### a. Experience\n",
    "- We will denote by $\\mathbb{1}_i \\in \\mathbb{R}^i $ the vector of ones $\\forall i \\in \\mathbb{N}$ \n",
    "- $\\mathscr{A}\\left(E,F\\right)$ is the set of affine transformations between $E$ and $F$\n",
    "- Let $m\\in \\mathbb{N}^*$ the number of features.\n",
    "- Let $n \\in \\mathbb{N}^*$ the number of samples\n",
    "- Let $X \\in E^n$ be a tuple of input samples \n",
    "- Let $y \\in \\{-1,1\\}^n$ be a tuple of ouput samples\n",
    "### b. Maximum Margin classifier\n",
    "- We w\n",
    "### c. Kernel\n",
    "- We will denote by kernel every function $K\\in\\mathscr{F}\\left(E,\\mathbb{R}\\right)$ satisfying:\n",
    "$$\\forall p\\in\\mathbb{N},\\forall x_1,\\dots,x_p\\in E,\\forall c_1,\\dots,c_p\\in\\mathbb{R},\\sum_{i,j\\in\\{1,\\dots,p\\}}c_ic_jK(x_i,x_j)\\ge 0$$\n",
    "In other words, $\\forall p\\in\\mathbb{N},\\forall x_1,\\dots,x_p\\in E$, the matrix $\\left(\\mathcal{K}_{i,j}\\right)_{i,j\\in\\{1,\\dots,p\\}}=\\left(K(x_i,x_j)\\right)_{i,j\\in\\{1,\\dots,p\\}}$ is positive semi definite\n",
    "We may consider kernels as a generalization of inner products\n",
    "## 2. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1=2000\n",
    "n2=2000\n",
    "R=np.r_[np.random.normal(1,1,n1),np.random.normal(10,2,n2)]\n",
    "A=np.random.uniform(0,2*np.pi,n1+n2)\n",
    "X=np.zeros([n1+n2,2])\n",
    "X[:,0]=R*np.cos(A)\n",
    "X[:,1]=R*np.sin(A)\n",
    "\n",
    "#X=np.c_[X[:,0]*np.cos(X[:,1]),X[:,0]*np.sin(X[:,1])]\n",
    "Y=np.array([(i<n1)*2-1 for i in range(n1+n2)])\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y)\n",
    "\n",
    "fig,ax = plt.subplots(5,sharex=True,sharey=True,figsize=(8,16))\n",
    "sns.scatterplot(x=X[:,0], y=X[:,1], hue=Y,ax=ax[0],palette=[\"g\",\"r\"]);\n",
    "ax[0].set_title(\"Lineary separable data\")\n",
    "alpha_range=[0,0.01,1,100]\n",
    "svc=[SupportVectorClassifier(u,K=lambda U,V:np.exp(-U@V.T)) for u in alpha_range]\n",
    "\n",
    "for i in range(4):\n",
    "    svc[i].fit(X_train,Y_train)\n",
    "    sns.scatterplot(x=X_test[:,0], y=X_test[:,1], hue=Y_test,ax=ax[i+1],palette=[\"g\",\"r\"]);\n",
    "    x_limits=np.array([np.min(X[:,0]),np.max(X[:,0])])\n",
    "  \n",
    "    ax[i+1].set_title(\"$\\\\alpha={}$\\tAccuarcy: {}\".format(alpha_range[i],svc[i].score(X_test,Y_test)))\n",
    "    ax[i+1].axis('square')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
